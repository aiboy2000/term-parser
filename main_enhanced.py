"""
Enhanced version with file upload support for terms management
Supports markdown, text, and CSV file uploads from RAG systems
"""
from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn
import re
import json
import csv
import io
from typing import List, Optional, Dict
from pathlib import Path

app = FastAPI(
    title="Enhanced Term Extractor API",
    description="å»ºç­‘ä¸šä¸“ä¸šæœ¯è¯­æŠ½å–API - æ”¯æŒæ–‡ä»¶ä¸Šä¼ å’Œæœ¯è¯­ç®¡ç†",
    version="2.0.0-enhanced"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ•°æ®æ¨¡å‹
class ExtractRequest(BaseModel):
    text: str
    min_confidence: Optional[float] = 0.5

class TermInfo(BaseModel):
    term: str
    confidence: float
    category: str

class ExtractResponse(BaseModel):
    terms: List[TermInfo]
    count: int

class SearchRequest(BaseModel):
    query: str
    limit: Optional[int] = 10

class AddTermRequest(BaseModel):
    term: str
    category: str
    aliases: Optional[List[str]] = []
    confidence: Optional[float] = 0.9

class UploadResponse(BaseModel):
    message: str
    processed_count: int
    added_terms: List[str]
    skipped_terms: List[str]

class TermManagementResponse(BaseModel):
    message: str
    affected_count: int

# æŒä¹…åŒ–å­˜å‚¨æ–‡ä»¶
TERMS_STORAGE_FILE = "custom_terms.json"

# å»ºç­‘ä¸šä¸“ä¸šæœ¯è¯­è¯å…¸ï¼ˆé»˜è®¤å†…ç½®ï¼‰
DEFAULT_CONSTRUCTION_TERMS = {
    "é‰„ç­‹ã‚³ãƒ³ã‚¯ãƒªãƒ¼ãƒˆ": {"aliases": ["RC", "é‰„ã‚³ãƒ³"], "category": "æ§‹é€ "},
    "ãƒ—ãƒ¬ã‚¹ãƒˆãƒ¬ã‚¹ãƒˆã‚³ãƒ³ã‚¯ãƒªãƒ¼ãƒˆ": {"aliases": ["PC"], "category": "æ§‹é€ "},
    "é‰„éª¨é‰„ç­‹ã‚³ãƒ³ã‚¯ãƒªãƒ¼ãƒˆ": {"aliases": ["SRC"], "category": "æ§‹é€ "},
    "åŸºç¤å·¥äº‹": {"aliases": ["åŸºç¤"], "category": "æ–½å·¥"},
    "èº¯ä½“å·¥äº‹": {"aliases": ["èº¯ä½“"], "category": "æ–½å·¥"},
    "ä»•ä¸Šå·¥äº‹": {"aliases": ["ä»•ä¸Šã’"], "category": "æ–½å·¥"},
    "ç©ºèª¿è¨­å‚™": {"aliases": ["ç©ºèª¿", "ã‚¨ã‚¢ã‚³ãƒ³"], "category": "è¨­å‚™"},
    "çµ¦æ’æ°´è¨­å‚™": {"aliases": ["çµ¦æ’æ°´"], "category": "è¨­å‚™"},
    "é›»æ°—è¨­å‚™": {"aliases": ["é›»æ°—"], "category": "è¨­å‚™"},
    "å“è³ªç®¡ç†": {"aliases": ["å“è³ª"], "category": "ç®¡ç†"},
    "å®‰å…¨ç®¡ç†": {"aliases": ["å®‰å…¨"], "category": "ç®¡ç†"},
    "æ–½å·¥ç®¡ç†": {"aliases": ["æ–½å·¥"], "category": "ç®¡ç†"},
    "è€éœ‡æ§‹é€ ": {"aliases": ["è€éœ‡"], "category": "æ§‹é€ "},
    "å…éœ‡æ§‹é€ ": {"aliases": ["å…éœ‡"], "category": "æ§‹é€ "},
    "åˆ¶éœ‡æ§‹é€ ": {"aliases": ["åˆ¶éœ‡"], "category": "æ§‹é€ "},
}

# åŠ¨æ€æœ¯è¯­è¯å…¸ï¼ˆå¯ä¿®æ”¹ï¼‰
CONSTRUCTION_TERMS = DEFAULT_CONSTRUCTION_TERMS.copy()

def load_custom_terms():
    """ä»æ–‡ä»¶åŠ è½½è‡ªå®šä¹‰æœ¯è¯­"""
    global CONSTRUCTION_TERMS
    try:
        if Path(TERMS_STORAGE_FILE).exists():
            with open(TERMS_STORAGE_FILE, 'r', encoding='utf-8') as f:
                custom_terms = json.load(f)
                CONSTRUCTION_TERMS.update(custom_terms)
                print(f"ğŸ“š åŠ è½½äº† {len(custom_terms)} ä¸ªè‡ªå®šä¹‰æœ¯è¯­")
    except Exception as e:
        print(f"âš ï¸ åŠ è½½è‡ªå®šä¹‰æœ¯è¯­å¤±è´¥: {e}")

def save_custom_terms():
    """ä¿å­˜è‡ªå®šä¹‰æœ¯è¯­åˆ°æ–‡ä»¶"""
    try:
        # åªä¿å­˜éé»˜è®¤çš„æœ¯è¯­
        custom_terms = {k: v for k, v in CONSTRUCTION_TERMS.items() 
                       if k not in DEFAULT_CONSTRUCTION_TERMS}
        with open(TERMS_STORAGE_FILE, 'w', encoding='utf-8') as f:
            json.dump(custom_terms, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ ä¿å­˜äº† {len(custom_terms)} ä¸ªè‡ªå®šä¹‰æœ¯è¯­")
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜è‡ªå®šä¹‰æœ¯è¯­å¤±è´¥: {e}")

def parse_text_file(content: str) -> List[Dict]:
    """è§£ætextæ–‡ä»¶å†…å®¹æå–æœ¯è¯­"""
    terms = []
    lines = content.strip().split('\n')
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # æ”¯æŒå¤šç§æ ¼å¼ï¼š
        # 1. ç®€å•æœ¯è¯­ï¼šæœ¯è¯­å
        # 2. å¸¦åˆ†ç±»ï¼šæœ¯è¯­å|åˆ†ç±»
        # 3. å¸¦åˆ«åï¼šæœ¯è¯­å|åˆ†ç±»|åˆ«å1,åˆ«å2
        parts = line.split('|')
        
        term = parts[0].strip()
        category = parts[1].strip() if len(parts) > 1 else "ä¸€èˆ¬"
        aliases = [a.strip() for a in parts[2].split(',')] if len(parts) > 2 else []
        
        if term:
            terms.append({
                "term": term,
                "category": category,
                "aliases": aliases
            })
    
    return terms

def parse_csv_file(content: str) -> List[Dict]:
    """è§£æCSVæ–‡ä»¶å†…å®¹æå–æœ¯è¯­"""
    terms = []
    
    try:
        reader = csv.DictReader(io.StringIO(content))
        
        for row in reader:
            # æ”¯æŒçš„CSVåˆ—åï¼ˆçµæ´»åŒ¹é…ï¼‰
            term = (row.get('term') or row.get('æœ¯è¯­') or 
                   row.get('å°ˆé–€ç”¨èª') or row.get('åç§°') or "").strip()
            
            category = (row.get('category') or row.get('åˆ†ç±»') or 
                       row.get('ã‚«ãƒ†ã‚´ãƒª') or row.get('é¡åˆ¥') or "ä¸€èˆ¬").strip()
            
            aliases_str = (row.get('aliases') or row.get('åˆ«å') or 
                          row.get('ã‚¨ã‚¤ãƒªã‚¢ã‚¹') or row.get('åˆ¥ç¨±') or "")
            
            aliases = [a.strip() for a in aliases_str.split(',') if a.strip()]
            
            if term:
                terms.append({
                    "term": term,
                    "category": category,
                    "aliases": aliases
                })
                
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"CSVè§£æé”™è¯¯: {e}")
    
    return terms

def parse_markdown_file(content: str) -> List[Dict]:
    """è§£æMarkdownæ–‡ä»¶å†…å®¹æå–æœ¯è¯­"""
    terms = []
    current_category = "ä¸€èˆ¬"
    
    lines = content.split('\n')
    
    for line in lines:
        line = line.strip()
        
        # æ£€æµ‹æ ‡é¢˜ä½œä¸ºåˆ†ç±»
        if line.startswith('#'):
            current_category = line.lstrip('#').strip()
            continue
        
        # æ£€æµ‹åˆ—è¡¨é¡¹ä½œä¸ºæœ¯è¯­
        if line.startswith('-') or line.startswith('*'):
            term_line = line.lstrip('-*').strip()
            
            # æ”¯æŒæ ¼å¼ï¼š- æœ¯è¯­å (åˆ«å1, åˆ«å2)
            if '(' in term_line and ')' in term_line:
                term = term_line.split('(')[0].strip()
                aliases_part = term_line.split('(')[1].split(')')[0]
                aliases = [a.strip() for a in aliases_part.split(',')]
            else:
                term = term_line
                aliases = []
            
            if term:
                terms.append({
                    "term": term,
                    "category": current_category,
                    "aliases": aliases
                })
        
        # æ£€æµ‹è¡¨æ ¼è¡Œ
        elif '|' in line and not line.startswith('|---'):
            parts = [p.strip() for p in line.split('|')]
            if len(parts) >= 3:  # è‡³å°‘æœ‰æœ¯è¯­å’Œåˆ†ç±»åˆ—
                term = parts[1] if len(parts) > 1 else ""
                category = parts[2] if len(parts) > 2 else current_category
                aliases_str = parts[3] if len(parts) > 3 else ""
                aliases = [a.strip() for a in aliases_str.split(',') if a.strip()]
                
                if term and term != "æœ¯è¯­":
                    terms.append({
                        "term": term,
                        "category": category,
                        "aliases": aliases
                    })
    
    return terms

def add_term_to_dictionary(term: str, category: str, aliases: List[str] = None) -> bool:
    """æ·»åŠ æœ¯è¯­åˆ°è¯å…¸"""
    if aliases is None:
        aliases = []
        
    if term not in CONSTRUCTION_TERMS:
        CONSTRUCTION_TERMS[term] = {
            "category": category,
            "aliases": aliases
        }
        return True
    return False

# æ­£è§„è¡¨è¾¾å¼æ¨¡å¼
TERM_PATTERNS = [
    (r'[ã‚¡-ãƒ´ãƒ¼]{3,}', "ã‚«ã‚¿ã‚«ãƒŠ", 0.7),  # ã‚«ã‚¿ã‚«ãƒŠç”¨è¯­
    (r'[ä¸€-é¾¯]{2,}[å·¥äº‹|æ–½å·¥|æ§‹é€ |ææ–™|è¨­å‚™|ç®¡ç†]', "å»ºç¯‰å°‚é–€", 0.9),  # å»ºç­‘ä¸“é—¨ç”¨è¯­
    (r'[A-Z]{2,}', "ç•¥èª", 0.8),  # ç•¥è¯­
    (r'\d+[ç´š|ç¨®|å·|å‹|ãœ|cm|m|éš]', "è¦æ ¼", 0.8),  # è§„æ ¼ãƒ»å°ºå¯¸
    (r'[ä¸€-é¾¯]{2,}[æ€§|åº¦|ç‡|é‡|å€¤]', "æ€§èƒ½", 0.6),  # æ€§èƒ½ç›¸å…³
]

def extract_terms_minimal(text: str, min_confidence: float = 0.5) -> List[dict]:
    """è½»é‡ç‰ˆæœ¯è¯­æŠ½å–"""
    found_terms = []
    seen_terms = set()
    
    # 1. æ—¢çŸ¥çš„ä¸“é—¨ç”¨è¯­æ£€æŸ¥
    for term, info in CONSTRUCTION_TERMS.items():
        if term in text and term not in seen_terms:
            seen_terms.add(term)
            found_terms.append({
                "term": term,
                "confidence": 0.95,
                "category": info["category"]
            })
        
        # åˆ«åä¹Ÿæ£€æŸ¥
        for alias in info["aliases"]:
            if alias in text and alias not in seen_terms:
                seen_terms.add(alias)
                found_terms.append({
                    "term": alias,
                    "confidence": 0.85,
                    "category": info["category"]
                })
    
    # 2. æ¨¡å¼åŒ¹é…
    for pattern, category, confidence in TERM_PATTERNS:
        matches = re.findall(pattern, text)
        for match in matches:
            if match not in seen_terms and len(match) >= 2:
                seen_terms.add(match)
                found_terms.append({
                    "term": match,
                    "confidence": confidence,
                    "category": category
                })
    
    # 3. å¯ä¿¡åº¦è¿‡æ»¤
    return [term for term in found_terms if term["confidence"] >= min_confidence]

def search_terms(query: str, limit: int = 10) -> List[dict]:
    """æœ¯è¯­æ£€ç´¢"""
    results = []
    query_lower = query.lower()
    
    # æ—¢çŸ¥çš„æœ¯è¯­æ£€ç´¢
    for term, info in CONSTRUCTION_TERMS.items():
        if query in term or query_lower in term.lower():
            results.append({
                "term": term,
                "confidence": 0.95,
                "category": info["category"]
            })
        
        # åˆ«åæ£€ç´¢
        for alias in info["aliases"]:
            if query in alias or query_lower in alias.lower():
                results.append({
                    "term": alias,
                    "confidence": 0.85,
                    "category": info["category"]
                })
    
    return results[:limit]

# åŸæœ‰APIç«¯ç‚¹
@app.post("/api/v1/extract", response_model=ExtractResponse)
async def extract_terms(request: ExtractRequest):
    """ä»æ–‡æœ¬ä¸­æŠ½å–å»ºç­‘ä¸“é—¨ç”¨è¯­"""
    try:
        terms = extract_terms_minimal(request.text, request.min_confidence)
        
        term_infos = [
            TermInfo(
                term=t["term"],
                confidence=t["confidence"],
                category=t["category"]
            )
            for t in terms
        ]
        
        return ExtractResponse(
            terms=term_infos,
            count=len(term_infos)
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æŠ½å–é”™è¯¯: {str(e)}")

@app.post("/api/v1/search")
async def search_construction_terms(request: SearchRequest):
    """å»ºç­‘ä¸“é—¨ç”¨è¯­æ£€ç´¢"""
    try:
        results = search_terms(request.query, request.limit)
        
        return {
            "query": request.query,
            "results": results,
            "count": len(results)
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ£€ç´¢é”™è¯¯: {str(e)}")

@app.get("/api/v1/terms")
async def list_all_terms():
    """æ˜¾ç¤ºå…¨éƒ¨ç™»å½•çš„ä¸“é—¨ç”¨è¯­"""
    terms = []
    for term, info in CONSTRUCTION_TERMS.items():
        terms.append({
            "term": term,
            "category": info["category"],
            "aliases": info["aliases"]
        })
    
    return {
        "terms": terms,
        "count": len(terms)
    }

# æ–°å¢APIç«¯ç‚¹
@app.post("/api/v1/upload", response_model=UploadResponse)
async def upload_terms_file(file: UploadFile = File(...)):
    """ä¸Šä¼ æœ¯è¯­æ–‡ä»¶ (æ”¯æŒ .txt, .csv, .md æ ¼å¼)"""
    try:
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        file_extension = Path(file.filename).suffix.lower()
        if file_extension not in ['.txt', '.csv', '.md']:
            raise HTTPException(
                status_code=400, 
                detail="ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ã€‚è¯·ä¸Šä¼  .txt, .csv æˆ– .md æ–‡ä»¶"
            )
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = await file.read()
        content_str = content.decode('utf-8')
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹è§£æ
        if file_extension == '.txt':
            parsed_terms = parse_text_file(content_str)
        elif file_extension == '.csv':
            parsed_terms = parse_csv_file(content_str)
        elif file_extension == '.md':
            parsed_terms = parse_markdown_file(content_str)
        
        # æ·»åŠ æœ¯è¯­åˆ°è¯å…¸
        added_terms = []
        skipped_terms = []
        
        for term_data in parsed_terms:
            if add_term_to_dictionary(
                term_data['term'], 
                term_data['category'], 
                term_data['aliases']
            ):
                added_terms.append(term_data['term'])
            else:
                skipped_terms.append(term_data['term'])
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        save_custom_terms()
        
        return UploadResponse(
            message=f"æˆåŠŸå¤„ç† {file.filename}",
            processed_count=len(parsed_terms),
            added_terms=added_terms,
            skipped_terms=skipped_terms
        )
        
    except UnicodeDecodeError:
        raise HTTPException(status_code=400, detail="æ–‡ä»¶ç¼–ç é”™è¯¯ï¼Œè¯·ä½¿ç”¨UTF-8ç¼–ç ")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶å¤„ç†é”™è¯¯: {str(e)}")

@app.post("/api/v1/terms/add", response_model=TermManagementResponse)
async def add_single_term(request: AddTermRequest):
    """æ‰‹åŠ¨æ·»åŠ å•ä¸ªæœ¯è¯­"""
    try:
        success = add_term_to_dictionary(
            request.term, 
            request.category, 
            request.aliases
        )
        
        if success:
            save_custom_terms()
            return TermManagementResponse(
                message=f"æˆåŠŸæ·»åŠ æœ¯è¯­: {request.term}",
                affected_count=1
            )
        else:
            return TermManagementResponse(
                message=f"æœ¯è¯­å·²å­˜åœ¨: {request.term}",
                affected_count=0
            )
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ·»åŠ æœ¯è¯­å¤±è´¥: {str(e)}")

@app.delete("/api/v1/terms/delete/{term}")
async def delete_term(term: str):
    """åˆ é™¤æœ¯è¯­"""
    try:
        if term in CONSTRUCTION_TERMS and term not in DEFAULT_CONSTRUCTION_TERMS:
            del CONSTRUCTION_TERMS[term]
            save_custom_terms()
            return TermManagementResponse(
                message=f"æˆåŠŸåˆ é™¤æœ¯è¯­: {term}",
                affected_count=1
            )
        elif term in DEFAULT_CONSTRUCTION_TERMS:
            return TermManagementResponse(
                message=f"æ— æ³•åˆ é™¤å†…ç½®æœ¯è¯­: {term}",
                affected_count=0
            )
        else:
            return TermManagementResponse(
                message=f"æœ¯è¯­ä¸å­˜åœ¨: {term}",
                affected_count=0
            )
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ é™¤æœ¯è¯­å¤±è´¥: {str(e)}")

@app.get("/api/v1/terms/stats")
async def get_terms_stats():
    """è·å–æœ¯è¯­ç»Ÿè®¡ä¿¡æ¯"""
    total_terms = len(CONSTRUCTION_TERMS)
    default_terms = len(DEFAULT_CONSTRUCTION_TERMS)
    custom_terms = total_terms - default_terms
    
    categories = {}
    for term, info in CONSTRUCTION_TERMS.items():
        cat = info['category']
        categories[cat] = categories.get(cat, 0) + 1
    
    return {
        "total_terms": total_terms,
        "default_terms": default_terms,
        "custom_terms": custom_terms,
        "categories": categories,
        "storage_file": TERMS_STORAGE_FILE
    }

@app.get("/api/v1/upload/formats")
async def get_supported_formats():
    """è·å–æ”¯æŒçš„æ–‡ä»¶æ ¼å¼è¯´æ˜"""
    return {
        "supported_formats": [
            {
                "format": ".txt",
                "description": "çº¯æ–‡æœ¬æ ¼å¼",
                "example": "æœ¯è¯­å|åˆ†ç±»|åˆ«å1,åˆ«å2"
            },
            {
                "format": ".csv",
                "description": "CSVè¡¨æ ¼æ ¼å¼",
                "columns": ["term", "category", "aliases"]
            },
            {
                "format": ".md",
                "description": "Markdownæ ¼å¼",
                "features": ["æ”¯æŒæ ‡é¢˜ä½œä¸ºåˆ†ç±»", "æ”¯æŒåˆ—è¡¨å’Œè¡¨æ ¼"]
            }
        ],
        "encoding": "UTF-8",
        "max_file_size": "10MB"
    }

@app.get("/")
async def root():
    return {
        "message": "å»ºç¯‰æ¥­ç•Œå°‚é–€ç”¨èªæŠ½å‡ºAPI - Enhanced",
        "version": "2.0.0-enhanced",
        "description": "æ”¯æŒæ–‡ä»¶ä¸Šä¼ å’Œæœ¯è¯­ç®¡ç†çš„å¢å¼ºç‰ˆ",
        "features": [
            "ä¸“é—¨ç”¨è¯­æŠ½å–",
            "ç”¨è¯­æ£€ç´¢",
            "åˆ†ç±»",
            "æ–‡ä»¶ä¸Šä¼  (txt/csv/md)",
            "æœ¯è¯­ç®¡ç†",
            "ä¸RAGç³»ç»Ÿé›†æˆ"
        ],
        "endpoints": {
            "extract": "/api/v1/extract",
            "search": "/api/v1/search", 
            "list_terms": "/api/v1/terms",
            "upload_file": "/api/v1/upload",
            "add_term": "/api/v1/terms/add",
            "delete_term": "/api/v1/terms/delete/{term}",
            "stats": "/api/v1/terms/stats",
            "formats": "/api/v1/upload/formats"
        }
    }

@app.get("/health")
async def health():
    return {"status": "healthy", "python_version": "3.12+", "version": "enhanced"}

@app.get("/demo")
async def demo():
    """ãƒ‡ãƒ¢ç”¨ã‚µãƒ³ãƒ—ãƒ«"""
    sample_text = "æœ¬å·¥äº‹ã¯é‰„ç­‹ã‚³ãƒ³ã‚¯ãƒªãƒ¼ãƒˆé€ ã®å»ºç¯‰ç‰©ã®åŸºç¤å·¥äº‹ã‚’å«ã¿ã¾ã™ã€‚RCæ§‹é€ ã§è€éœ‡æ€§èƒ½ã‚’ç¢ºä¿ã—ã€å“è³ªç®¡ç†ã‚’å¾¹åº•ã—ã¾ã™ã€‚"
    
    terms = extract_terms_minimal(sample_text)
    
    return {
        "sample_text": sample_text,
        "extracted_terms": terms,
        "usage": {
            "extract": "POST /api/v1/extract with {'text': 'your text here'}",
            "upload": "POST /api/v1/upload with file upload",
            "add_term": "POST /api/v1/terms/add with term data"
        }
    }

if __name__ == "__main__":
    # å¯åŠ¨æ—¶åŠ è½½è‡ªå®šä¹‰æœ¯è¯­
    load_custom_terms()
    
    print("ğŸ—ï¸  å»ºç¯‰æ¥­ç•Œå°‚é–€ç”¨èªæŠ½å‡ºAPI (Enhanced) starting...")
    print("ğŸ“¡ API docs: http://localhost:8000/docs")
    print("ğŸ¯ Demo: http://localhost:8000/demo")
    print("ğŸ“ Upload: http://localhost:8000/api/v1/upload")
    print("ğŸ“Š Stats: http://localhost:8000/api/v1/terms/stats")
    
    uvicorn.run(
        "main_enhanced:app",
        host="0.0.0.0",
        port=8000,
        reload=True
    )